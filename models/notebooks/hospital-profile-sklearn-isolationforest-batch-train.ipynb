{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# call the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,OneHotEncoder\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize spark and spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"hospital-profile-sklearn-isolationforest-batch-train\")\\\n",
    ".master(\"local[*]\")\\\n",
    ".config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\\\n",
    ".enableHiveSupport()\\\n",
    ".getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenant_name = \"claim\"\n",
    "time_window=\"day\"\n",
    "entity_type=\"hospital\"\n",
    "anomaly_type=\"profile\"\n",
    "model_type=\"sklearn\"\n",
    "model_name=\"isolationforest\"\n",
    "\n",
    "BASE_PATH = \"/Users/tuhinsharma/Documents/sstech/\"+tenant_name\n",
    "ANOMALY_DATA_REPOSITORY = BASE_PATH + \"/models_data/data\"\n",
    "\n",
    "USER_PROFILE_DATA_PATH = ANOMALY_DATA_REPOSITORY + \"/{entity_type}/{anomaly_type}/{time_window}.json\"\n",
    "data_path = USER_PROFILE_DATA_PATH.format(entity_type=entity_type,anomaly_type=\"profile\",time_window=time_window)\n",
    "    \n",
    "ANOMALY_MODEL_REPOSITORY = BASE_PATH + \"/models_data/model\"\n",
    "PROFILE_ANOMALY_MODEL_PATH = ANOMALY_MODEL_REPOSITORY + \"/{entity_type}/{anomaly_type}/{time_window}/{model_type}/{model_name}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query for Hospital Profile data for time interval \"2019-06-21T12:41:20.000Z\" to \"2019-06-21T12:40:00.000Z\" for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------------+--------------------+-------------------+------------------+-----------------+\n",
      "|hospital_id|maximum_claim_amount|maximum_patient_age|minimum_claim_amount|minimum_patient_age|total_claim_amount|total_claim_count|\n",
      "+-----------+--------------------+-------------------+--------------------+-------------------+------------------+-----------------+\n",
      "|      HOSP1|             31990.0|                 69|              1660.0|                 21|          154150.0|               11|\n",
      "|     HOSP10|             43390.0|                 61|              2730.0|                 17|          139230.0|                7|\n",
      "|    HOSP100|             39280.0|                 64|              6790.0|                 19|          166680.0|               10|\n",
      "+-----------+--------------------+-------------------+--------------------+-------------------+------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = {\n",
    "  \"queryType\": \"groupBy\",\n",
    "  \"dataSource\": \"hospital_profile_two_minute\",\n",
    "  \"dimensions\": [\n",
    "    \"datetime\"\n",
    "  ],\n",
    "  \"aggregations\": [],\n",
    "  \"granularity\": \"all\",\n",
    "  \"postAggregations\": [],\n",
    "  \"intervals\": \"1901-01-01T00:00:00+00:00/2101-01-01T00:00:00+00:00\",\n",
    "  \"limitSpec\": {\n",
    "    \"type\": \"default\",\n",
    "    \"limit\": 2,\n",
    "    \"columns\": [\n",
    "      {\n",
    "        \"dimension\": \"datetime\",\n",
    "        \"direction\": \"descending\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "import requests\n",
    "r = requests.post('http://0.0.0.0:28082/druid/v2/', json=query)\n",
    "r.status_code\n",
    "datetime = r.json()[1][\"event\"][\"datetime\"]\n",
    "\n",
    "\n",
    "query = {\n",
    "   \"queryType\": \"select\",\n",
    "   \"dataSource\": \"hospital_profile_two_minute\",\n",
    "   \"descending\": \"false\",\n",
    "   \"metrics\":['hospital_id','maximum_claim_amount', 'maximum_patient_age', 'minimum_claim_amount',\n",
    "       'minimum_patient_age', 'total_claim_amount', 'total_claim_count'],\n",
    "   \"granularity\": \"all\",\n",
    "   \"intervals\": \"1901-01-01T00:00:00+00:00/2101-01-01T00:00:00+00:00\",\n",
    "    \"filter\": {\n",
    "    \"type\": \"selector\",\n",
    "    \"dimension\": \"datetime\",\n",
    "    \"value\": datetime\n",
    "  },\n",
    "   \"pagingSpec\":{\"pagingIdentifiers\": {}, \"threshold\":1000}\n",
    " }\n",
    "\n",
    "import requests\n",
    "r = requests.post('http://0.0.0.0:28082/druid/v2/', json=query)\n",
    "r.status_code\n",
    "query_result = r.json()[0][\"result\"]\n",
    "x = query_result[\"events\"]\n",
    "query_result = [i[\"event\"] for i in x]\n",
    "query_result\n",
    "\n",
    "hospital_profile_df = pd.DataFrame.from_records(query_result)[['hospital_id','maximum_claim_amount', 'maximum_patient_age', 'minimum_claim_amount',\n",
    "       'minimum_patient_age', 'total_claim_amount', 'total_claim_count']].copy()\n",
    "hospital_profile_df.head(10)\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([ StructField(\"hospital_id\", StringType(), True),\n",
    "                     StructField(\"maximum_claim_amount\", FloatType(), True)\n",
    "                       ,StructField(\"maximum_patient_age\", LongType(), True)\\\n",
    "                       ,StructField(\"minimum_claim_amount\", FloatType(), True)\\\n",
    "                       ,StructField(\"minimum_patient_age\", LongType(), True)\\\n",
    "                       ,StructField(\"total_claim_amount\", FloatType(), True)\\\n",
    "                       ,StructField(\"total_claim_count\", LongType(), True)\\\n",
    "                       ])\n",
    "hospital_profile_sdf = spark.createDataFrame(hospital_profile_df,schema=schema)\n",
    "hospital_profile_sdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"timestamp\": \"2019-07-05T15:28:00.000Z\", \"datetime\": \"2019-07-05T15:28:00.000Z\", \"timestamp_start\": \"1562340360000\", \"timestamp_end\": \"1562340480000\", \"hospital_id\": \"HOSP1\", \"total_claim_amount\": 154150.0, \"total_claim_count\": 11, \"minimum_claim_amount\": 1660.0, \"maximum_claim_amount\": 31990.0, \"minimum_patient_age\": 21, \"maximum_patient_age\": 69}'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.dumps(query_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hospital_id', 'maximum_claim_amount', 'maximum_patient_age',\n",
       "       'minimum_claim_amount', 'minimum_patient_age', 'total_claim_amount',\n",
       "       'total_claim_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_profile_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_profile_df= hospital_profile_sdf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the columns on which Model shall be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_colnames = []\n",
    "num_colnames = ['maximum_claim_amount', 'maximum_patient_age', 'minimum_claim_amount',\n",
    "       'minimum_patient_age', 'total_claim_amount', 'total_claim_count']\n",
    "num_data = hospital_profile_df[num_colnames].values.astype(np.float64)\n",
    "if len(cat_colnames)>0:\n",
    "    cat_data = hospital_profile_df[cat_colnames].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IsolationForest Pipelinemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = None\n",
    "if len(num_colnames)>0:\n",
    "    standard_scaler = StandardScaler()\n",
    "    num_data_normalized = standard_scaler.fit_transform(num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = None\n",
    "if len(cat_colnames)>0:\n",
    "    one_hot_encoder = OneHotEncoder(categories='auto')\n",
    "    cat_data_encoded = one_hot_encoder.fit_transform(cat_data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(num_colnames)>0 and len(cat_colnames)>0:\n",
    "    data = np.concatenate((num_data_normalized,cat_data_encoded),axis=1)\n",
    "elif len(cat_colnames):\n",
    "    data = cat_data_encoded\n",
    "elif len(num_colnames):\n",
    "    data = num_data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolation_forest = IsolationForest(behaviour='new',n_estimators=1000,max_samples=0.3,max_features=min(4,len(num_colnames+cat_colnames)),bootstrap=True,\n",
    "                                         contamination=\"auto\",\n",
    "                                         random_state=42)\n",
    "isolation_forest_model = isolation_forest.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_profile_df[\"score\"] = isolation_forest_model.decision_function(data).reshape(-1, 1)*-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scoring pipelinemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = hospital_profile_df[\"score\"].values.reshape(-1, 1)\n",
    "minmax_scaler = MinMaxScaler(feature_range=(0,100))\n",
    "scoring_pipeline = Pipeline(steps=[(\"MinMaxScaler\",minmax_scaler)])\n",
    "scoring_pipeline_model = scoring_pipeline.fit(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = PROFILE_ANOMALY_MODEL_PATH.format(entity_type=entity_type,anomaly_type=anomaly_type,time_window=time_window,\\\n",
    "                                 model_type=model_type,model_name=model_name)\n",
    "\n",
    "os.system(\"hdfs dfs -rm -r \"+model_path)\n",
    "os.system(\"rm -rf \"+model_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Clustering Pipelinemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolation_forest_model_rdd = sc.parallelize([{\"cat_colnames\":cat_colnames,\"num_colnames\":num_colnames,\"standard_scaler\":standard_scaler,\"one_hot_encoder\":one_hot_encoder,\"isolation_forest_model\":isolation_forest_model}])\n",
    "isolation_forest_model_rdd.saveAsPickleFile(model_path+\"/if_pipeline_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Scoring Pipelinemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_pipeline_model_rdd = sc.parallelize([scoring_pipeline_model])\n",
    "scoring_pipeline_model_rdd.saveAsPickleFile(model_path+\"/scoring_pipeline_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataAndAnalytics-v3",
   "language": "python",
   "name": "dataandanalytics-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
